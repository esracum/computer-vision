{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/esracum/face-detect-with-opencv-dnn?scriptVersionId=293472348\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Face Detection on YouTube Video Segments\n\nThis notebook implements a high-performance face detection pipeline using OpenCV's Deep Neural Network (DNN) module. this project leverages the ResNet-10 SSD architecture to analyze specific video intervals.","metadata":{}},{"cell_type":"markdown","source":"# Step 1. Setup and Asset Acquisition\n### In this stage, we download the necessary model configuration (.prototxt) and the pre-trained weights (.caffemodel).\n","metadata":{}},{"cell_type":"code","source":"!pip install -U yt-dlp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:44:29.179529Z","iopub.execute_input":"2026-01-21T10:44:29.179887Z","iopub.status.idle":"2026-01-21T10:44:34.127251Z","shell.execute_reply.started":"2026-01-21T10:44:29.179857Z","shell.execute_reply":"2026-01-21T10:44:34.126031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 2: Download Assets and Video\n### It can be downloaded manually from -> [GitHub Source Link](http:///github.com/opencv/opencv/tree/master/samples/dnn/face_detector)","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport sys\nimport yt_dlp\nfrom zipfile import ZipFile\nfrom urllib.request import urlretrieve\n\n# ========================-DOWNLOADİNG ASSETS-========================\n\ndef download_and_unzip(url, save_path):\n    if not os.path.exists(save_path):\n        print(f\"Downloading and extracting assets....\", end=\"\")\n        urlretrieve(url, save_path)\n        with ZipFile(save_path) as z:\n            z.extractall(os.path.split(save_path)[0])\n        print(\"Done\")\n\nURL = r\"https://www.dropbox.com/s/efitgt363ada95a/opencv_bootcamp_assets_12.zip?dl=1\"\nasset_zip_path = os.path.join(os.getcwd(), \"opencv_bootcamp_assets_12.zip\")\ndownload_and_unzip(URL, asset_zip_path)\n\n# --- YOUTUBE TEST VİDEO DOWNLOAD ---\n\nyoutube_url = \"https://www.youtube.com/watch?v=qB6BGsCqAgA\"\ndef download_youtube_video(url):\n    \n    ydl_opts = {\n        'format': '18', \n        'outtmpl': 'input_video.mp4', \n        'quiet': True,\n        'no_warnings': True, \n        'nocheckcertificate': True \n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n    return \"input_video.mp4\"\n\nvideo_path = download_youtube_video(youtube_url)\nprint(\"Preparation is complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:44:34.129741Z","iopub.execute_input":"2026-01-21T10:44:34.130122Z","iopub.status.idle":"2026-01-21T10:44:35.665076Z","shell.execute_reply.started":"2026-01-21T10:44:34.130078Z","shell.execute_reply":"2026-01-21T10:44:35.663723Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 3: Face Detection and Video Transcoding\n\n.prototxt: Defines the network architecture (how the layers are arranged).\n\n.caffemodel: Contains the trained weights (learned facial features).\n\n\n\n---","metadata":{}},{"cell_type":"markdown","source":"### -----> mean = [104, 117, 123]\n\n\nThese numbers were not chosen randomly. The model we used was trained on a massive dataset called ImageNet, which contains millions of photos.\n\n104: This is the average of the Blue channels in all those millions of images.\n\n117: This is the average of the Green channels in all those images.\n\n123: This is the average of the Red channels in all those images. (Note: Since OpenCV uses the BGR format, the order is Blue-Green-Red.)\n\nIf we remove these \"average\" values, we essentially ignore the overall brightness level of the image (whether it is bright or dark). Thus, the AI ​​focuses on the features and shape of the face, rather than the intensity of the light.","metadata":{}},{"cell_type":"code","source":"# Set video source\nsource = cv2.VideoCapture(video_path)\n\nif not source.isOpened():\n    print(f\"Error: Video file could not be opened!\")\nelse:\n    # --- SETTİNGS ---\n    start_second = 45  # From which second of the video should it start?\n    duration = 15     # For how many seconds should the operation last?\n    \n    # Jump to starting point (In milliseconds: seconds * 1000)\n    source.set(cv2.CAP_PROP_POS_MSEC, start_second * 1000)\n    \n    # Get video information\n    frame_width = int(source.get(3))\n    frame_height = int(source.get(4))\n    fps = int(source.get(cv2.CAP_PROP_FPS))\n    limit_frames = fps * duration # İşlenecek toplam kare sayısı\n    \n    # Temporary raw video file (with mp4v)\n    out = cv2.VideoWriter('temp_output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n    net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000_fp16.caffemodel\")\n    \n    in_width, in_height = 300, 300\n    mean = [104, 117, 123]\n    conf_threshold = 0.7\n    frame_count = 0\n\n    print(f\"Processing {duration} seconds of video starting from the {start_second}. second...\")    \n    while True:\n        has_frame, frame = source.read()\n        \n        # The specified time (limit_frames) is the duration for which changes or the video ends.\n        if not has_frame or frame_count > limit_frames:\n            break\n        \n        frame_count += 1\n        frame = cv2.flip(frame, 1) # Mirrors the image \n        \n        # Create a 4D blob from a frame.\n        blob = cv2.dnn.blobFromImage(frame, 1.0, (in_width, in_height), mean, swapRB=False, crop=False)\n        net.setInput(blob)\n        detections = net.forward()\n\n        for i in range(detections.shape[2]):\n            confidence = detections[0, 0, i, 2]\n            if confidence > conf_threshold:\n                x1 = int(detections[0, 0, i, 3] * frame_width)\n                y1 = int(detections[0, 0, i, 4] * frame_height)\n                x2 = int(detections[0, 0, i, 5] * frame_width)\n                y2 = int(detections[0, 0, i, 6] * frame_height)\n                \n                # Drawing\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        \n        out.write(frame)\n\n    source.release()\n    out.release()\n    \n   \n    os.system(\"ffmpeg -y -i temp_output.mp4 -vcodec libx264 -loglevel quiet binnaz_output.mp4 > /dev/null 2>&1\")\n    \n    print(f\"Processing complete! The segment from {start_second} to {start_second + duration} seconds is ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:44:35.666332Z","iopub.execute_input":"2026-01-21T10:44:35.66666Z","iopub.status.idle":"2026-01-21T10:44:58.317441Z","shell.execute_reply.started":"2026-01-21T10:44:35.666628Z","shell.execute_reply":"2026-01-21T10:44:58.316481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 4: Display Result","metadata":{}},{"cell_type":"code","source":"from IPython.display import Video\nVideo(\"binnaz_output.mp4\", embed=True, width=700)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T10:44:58.319576Z","iopub.execute_input":"2026-01-21T10:44:58.319917Z","iopub.status.idle":"2026-01-21T10:44:58.349594Z","shell.execute_reply.started":"2026-01-21T10:44:58.319879Z","shell.execute_reply":"2026-01-21T10:44:58.34785Z"}},"outputs":[],"execution_count":null}]}